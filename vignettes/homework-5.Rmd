---
title: "Homework 5"
author: "Clint Guo"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: yes
  pdf_document:
    toc: yes
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Homework 5 vignette}
-->

```{r}
library(keras)
install_keras()
library(keras)
library(glmnet)
library(moments)
library(dplyr)
library(ggplot2)
library(tibble)

library(bis557)

set.seed(1222)
```

**1.** MNIST

```{r}
mnist <- dataset_mnist()
X_train <- mnist$train$x
y_train <- mnist$train$y
X_test <- mnist$test$x
y_test <- mnist$test$y

# As the dataset is too large, we have to sample it for an acceptable training time. 
train_idx <- sample(nrow(X_train), nrow(X_train) / 100)
test_idx <- sample(nrow(X_test), nrow(X_test) / 100)

X_train <- X_train[train_idx, , ]
y_train <- y_train[train_idx]
X_test <- X_test[test_idx, , ]
y_test <- y_test[test_idx]

X_train <- array_reshape(X_train, c(nrow(X_train), 28^2))
y_train <- factor(y_train)
X_test <- array_reshape(X_test, c(nrow(X_test), 28^2))
y_test <- factor(y_test)

fit <- cv.glmnet(X_train, y_train, family = "multinomial")
preds <- predict(fit$glmnet.fit, X_test, s = fit$lambda.min, 
                 type = "class")
tbl <- table(factor(preds), y_test)
cat("The out-of-sample prediction accuracy of LASSO: ", sum(diag(tbl)) / sum(tbl))
```

Let's explore some of the features that we can extract from the images. 

* moment

```{r}
moment <- apply(X_train, 1, moment)
tmp <- cbind(y_train, moment)
colnames(tmp) <- c("lab", "mom")
tmp <- data.frame(tmp)
tmp %>% group_by(lab) %>% summarise(avg = mean(mom)) %>% ggplot() +
  geom_col(aes(x = lab, y = avg)) + scale_x_discrete(limits=1:10) + 
  ylab("Skewness")
```

* skewness

```{r}
skewness <- apply(X_train, 1, skewness)
tmp <- cbind(y_train, skewness)
colnames(tmp) <- c("lab", "ske")
tmp <- data.frame(tmp)
tmp %>% group_by(lab) %>% summarise(avg = mean(ske)) %>% ggplot() +
  geom_col(aes(x = lab, y = avg)) + scale_x_discrete(limits=1:10) + 
  ylab("Skewness")
```

* kurtosis

```{r}
kurtosis <- apply(X_train, 1, kurtosis)
tmp <- cbind(y_train, kurtosis)
colnames(tmp) <- c("lab", "kur")
tmp <- data.frame(tmp)
tmp %>% group_by(lab) %>% summarise(avg = mean(kur)) %>% ggplot() +
  geom_col(aes(x = lab, y = avg)) + scale_x_discrete(limits=1:10) + 
  ylab("Skewness")
```

From the above plots, we can see that the features of different digits do differ to some extent. We add 'moment' and 'skewness' into our predictive model. 

```{r}
X_train2 <- cbind(X_train, moment, kurtosis)
X_test2 <- cbind(X_test, apply(X_test, 1, moment), apply(X_test, 1, kurtosis))

fit2 <- cv.glmnet(X_train2, y_train, family = "multinomial")
preds2 <- predict(fit2$glmnet.fit, X_test2, s = fit2$lambda.min, 
                  type = "class")
tbl2 <- table(factor(preds2), y_test)
cat("The out-of-sample prediction accuracy of LASSO (with moment & kurtosis): ",
    sum(diag(tbl2)) / sum(tbl2))
```

A 1% improvement!

**2.** CASL page 253, question 4

```{r, eval=FALSE}
data(emnist_train)
data(emnist_test)

X_train <- emnist_train[, 3:786]
y_train <- as.vector(emnist_train[, 2]) - 1
X_test <- emnist_test[, 3:786]
y_test <- as.vector(emnist_test[, 2]) - 1

X_train <- X_train / 255
X_test <- X_test / 255

X_train <- as.matrix(X_train)
X_train <- array_reshape(X_train, dim = c(nrow(X_train), 28, 28, 1))
X_test <- as.matrix(X_test)
X_test <- array_reshape(X_test, dim = c(nrow(X_test), 28, 28, 1))

# One hot
y_train <- to_categorical(y_train, 26L)
y_test <- to_categorical(y_test, 26L)
```

```{r, eval=FALSE}
model_emnist <- keras_model_sequential()
model_emnist %>% 
  layer_conv_2d(filters = 32, kernel_size = c(2, 2),padding = "same", 
                input_shape = c(28, 28, 1)) %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

model_emnist %>% compile(loss = 'categorical_crossentropy',
                         optimizer = optimizer_rmsprop(),
                         metrics = c('accuracy'))

history_emnist <- model_emnist %>% fit(X_train, y_train, epochs = 10,
                                       validation_data = list(X_test, y_test))
print(history_emnist)
```

Accuracy = 0.829, validation accuracy = 0.8739. We then carry out the following modifications: 
1. Change kernel sizes of the first two convolution layers to c(3, 3), and the last two to c(5, 5). 
2. Reduce drop out rate to 0.4.
3. Add a Batch Normalization layer between each convolution layer. 

```{r, eval=FALSE}
model_emnist2 <- keras_model_sequential()
model_emnist2 %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = "same", 
                input_shape = c(28, 28, 1)) %>%
  layer_activation(activation = "relu") %>%
  layer_batch_normalization() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(5, 5), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_conv_2d(filters = 32, kernel_size = c(5, 5), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

model_emnist2 %>% compile(loss = 'categorical_crossentropy',
                          optimizer = optimizer_rmsprop(),
                          metrics = c('accuracy'))

history_emnist2 <- model_emnist2 %>% fit(X_train, y_train, epochs = 10,
                                         validation_data = list(X_test, y_test))
print(history_emnist2)
```

Both the training and validation accuracy have improved (accuracy = 0.899, validation accuracy = 0.902).  

**3.** CASL page 253, question 8

```{r}
# Create list of weights to describe a dense neural network.
#
# Args:
#     sizes: A vector giving the size of each layer, including
#            the input and output layers.
#
# Returns:
#     A list containing initialized weights and biases.

nn_make_weights <- function(sizes) {
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)) {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]), 
                ncol = sizes[j], nrow = sizes[j + 1L])
    weights[[j]] <- list(w = w, b = rnorm(sizes[j + 1L]))
  }
  weights
}
```

```{r}
# Apply a rectified linear unit (ReLU) to a vector/matrix.
#
# Args: 
#     v: A numeric vector or matrix.
#
# Returns:
#     The original input with negative values truncated to zero.

ReLU <- function(v) {
  v[v < 0] <- 0
  v
}
```

```{r}
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     Sets positive values to 1 and negative values to zero.

ReLU_p <- function(v) {
  p <- v * 0
  p[v > 0] <- 1
  p
}
```

```{r}
# Derivative of the mean absolute deviance (MAD) function.
#
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MAD function.

mad_p <- function(y, a) {
  p <- rep(NA, length(a))
  for (i in 1:length(a)){
    if (y[i] <= a[i]) 
      p[i] <- 1
    else 
      p[i] <- -1
  }
  p
}
```

```{r}
# Derivative of the mean squared error (MSE) function.
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MSE function.

mse_p <- function(y, a) {
  1/2*(a - y)
}
```

```{r}
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     weights: A list created by nn_make_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and activations (a).

nn_forward_prop <- function(x, weights, sigma) {
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)) {
  a_j1 <- if(j == 1) x else a[[j - 1L]]
  z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
  a[[j]] <- if(j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z = z, a = a)
}
```

```{r}
# Apply backward propagation algorithm.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     y: A numeric vector representing one row of the response.
#     weights: A list created by nn_make_weights.
#     f_obj: Output of the function nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and activations (a).

nn_backward_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
  z <- f_obj$z
  a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))) {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]])
      } else {
        grad_z[[j]] <- (t(weights[[j + 1]]$w) %*% grad_z[[j + 1]]) * 
          sigma_p(z[[j]])
        }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
    }
  list(grad_z = grad_z, grad_w = grad_w)
}
```

```{r}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in the neural network.
#     epochs: Integer number of epochs to computer.
#     eta: Positive numeric learning rate.
#     f_p: Derivative of the loss function.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.

nn_sgd <- function(X, y, sizes, epochs, eta, f_p, weights=NULL) {
  if (is.null(weights)) {
    weights <- nn_make_weights(sizes)
  }
  
  for (epoch in seq_len(epochs)){
    for (i in seq_len(nrow(X))){ 
      f_obj <- nn_forward_prop(X[i,], weights, ReLU)
      b_obj <- nn_backward_prop(X[i,], y[i,], weights,
                                     f_obj, ReLU_p, f_p)
      for (j in seq_along(weights)){
        weights[[j]]$b <- weights[[j]]$b -eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w -eta * b_obj$grad_w[[j]]
      }
    }
  }
  weights
}
```

```{r}
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.

nn_predict <- function(weights, X_test) {
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- nn_forward_prop(X_test[i,], weights, ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
  }
  y_hat
}
```

We test the use of this function with a simulation containing several outliers.

```{r}
set.seed(1222)
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
y <- X[, 1, drop = FALSE]^2 + rnorm(1000, sd = 0.1)

# Adding several outliers
ind <- sample(seq_along(y), 100)
y[ind] <- c(runif(50, -10, -5), runif(50, 5, 10))

weights <- nn_sgd(X, y, sizes = c(1, 25, 1), epochs = 15, eta = 0.01, mad_p)
y_pred <- nn_predict(weights, X)

weights2 <- nn_sgd(X, y, sizes = c(1, 25, 1), epochs = 15, eta = 0.01, mse_p)
y_pred2 <- nn_predict(weights2, X)

df <- tibble(X = as.vector(X), y_pred = as.vector(y_pred), 
             y_pred2 = as.vector(y_pred2), y = as.vector(y))
ggplot(df) + geom_point(aes(x = X, y = y)) + ylim(c(-1, 2)) + 
  geom_point(aes(x = X, y = y_pred), color = "red") + 
  geom_point(aes(x = X, y = y_pred2), color = "blue") + 
  labs(x = "X", y = "True/Predicted Values", 
       subtitle="Black: True; Red: MAD Predicted; Blue: MSE Predicted")
```

For better visualization, the above plot does not show the large outliers. We can see that in terms of robustness, MAD (red line) performs better than MSE (blue line). 
